{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 6. Speeding up map and reduce with advanced parallelization\n",
    "====\n",
    "### Mastering Large Datasets with Python by JT Wolohan \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- N = 1 --\n",
      "Lazy map time:      8.000000000008e-06\n",
      "Parallel map time:  0.01324599999999998\n",
      "\n",
      "\n",
      "-- N = 10 --\n",
      "Lazy map time:      9.599999999998499e-05\n",
      "Parallel map time:  0.014952000000000076\n",
      "\n",
      "\n",
      "-- N = 100 --\n",
      "Lazy map time:      5.900000000003125e-05\n",
      "Parallel map time:  0.01502199999999998\n",
      "\n",
      "\n",
      "-- N = 1000 --\n",
      "Lazy map time:      0.0003989999999999272\n",
      "Parallel map time:  0.014475000000000016\n",
      "\n",
      "\n",
      "-- N = 10000 --\n",
      "Lazy map time:      0.0038730000000000153\n",
      "Parallel map time:  0.01732200000000006\n",
      "\n",
      "\n",
      "-- N = 100000 --\n",
      "Lazy map time:      0.03707399999999994\n",
      "Parallel map time:  0.02400800000000003\n",
      "\n",
      "\n",
      "-- N = 1000000 --\n",
      "Lazy map time:      0.199009\n",
      "Parallel map time:  0.13838499999999998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import clock, sleep\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def times_two(x):\n",
    "  return x*2+7\n",
    "\n",
    "\n",
    "def lazy_map(xs):\n",
    "  return list(map(times_two, xs))\n",
    "\n",
    "\n",
    "def parallel_map(xs, chunck=8500):\n",
    "  with Pool(2) as P:\n",
    "    x =  P.map(times_two, xs, chunck)\n",
    "  return x\n",
    "\n",
    "\n",
    "for i in range(0, 7):\n",
    "  N = 10**i\n",
    "  t1 = clock()\n",
    "  lazy_map(range(N))\n",
    "  lm_time = clock() - t1\n",
    "\n",
    "  t1 = clock()\n",
    "  parallel_map(range(N))\n",
    "  par_time = clock() - t1\n",
    "  print(\"\"\"\n",
    "-- N = {} --\n",
    "Lazy map time:      {}\n",
    "Parallel map time:  {}\n",
    "\"\"\".format(N, lm_time, par_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "chunksize   |  runtime\n",
      "-------------------------\n",
      "5           |   5.083\n",
      "50          |   1.431\n",
      "500         |   0.291\n",
      "5000        |   0.199\n",
      "50000       |   0.159\n",
      "500000      |   0.203\n",
      "5000000     |   0.182\n",
      "50000000    |   0.164\n",
      "500000000   |   0.157\n"
     ]
    }
   ],
   "source": [
    "from time import clock\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def times_two(x):\n",
    "  return x*2+7\n",
    "\n",
    "\n",
    "def parallel_map(xs, chunk_size=8500):\n",
    "  with Pool(2) as P:\n",
    "    x = P.map(times_two, xs, chunk_size)\n",
    "  return x\n",
    "\n",
    "\n",
    "print(\"\"\"\n",
    "{:<10}  |  {}\n",
    "-------------------------\"\"\".format(\"chunksize\", \"runtime\"))\n",
    "\n",
    "for i in range(0, 9):\n",
    "  N = 1000000\n",
    "  chunk_size = 5 * (10**i)\n",
    "\n",
    "  t1 = clock()\n",
    "  parallel_map(range(N), chunk_size)\n",
    "  parallel_time = clock() - t1\n",
    "\n",
    "  print(\"{:<10}  |   {:>0.3f}\".format(chunk_size, parallel_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from toolz.sandbox.parallel import fold\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def my_add(left, right):\n",
    "  return left+right\n",
    "\n",
    "\n",
    "with Pool() as P: \n",
    "    fold(my_add, range(500000), map=P.imap)\n",
    "\n",
    "print(reduce(my_add, range(500)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492, 494, 496, 498]\n"
     ]
    }
   ],
   "source": [
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from toolz.sandbox.parallel import fold\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def map_combination(left, right):\n",
    "  return left + right\n",
    "\n",
    "\n",
    "def keep_if_even(acc, nxt):\n",
    "    if nxt % 2 == 0:\n",
    "        return acc + [nxt]\n",
    "    else: return acc\n",
    "\n",
    "\n",
    "with Pool() as P:\n",
    "    fold(keep_if_even, range(500000), [],\n",
    "         map=P.imap, combine=map_combination)\n",
    "\n",
    "print(reduce(keep_if_even, range(500), []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 76, 2: 94, 3: 74, 4: 78, 5: 88, 6: 90}\n"
     ]
    }
   ],
   "source": [
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from toolz.sandbox.parallel import fold\n",
    "from random import choice\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def combine_counts(left, right):\n",
    "  unique_keys = set(left.keys()).union(set(right.keys()))\n",
    "  return {k:left.get(k, 0)+right.get(k, 0) for k in unique_keys}\n",
    "\n",
    "\n",
    "def make_counts(acc, nxt):\n",
    "    acc[nxt] = acc.get(nxt,0) + 1\n",
    "    return acc\n",
    "\n",
    "\n",
    "xs = (choice([1, 2, 3, 4, 5, 6]) for _ in range(500000))\n",
    "\n",
    "with Pool() as P:\n",
    "    fold(make_counts, xs, {},\n",
    "         map=P.imap, combine=combine_counts)\n",
    "\n",
    "print(reduce(make_counts, (choice([1, 2, 3, 4, 5, 6]) for _ in range(500)), {}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Naive Bayes\n",
    "**NB:** *This code ended up getting cut from the book. It implements the naive Bayes algorithm in parallel using map and reduce patterns. Feel free to read through it as a bonus.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import starmap, repeat\n",
    "from functools import reduce, partial\n",
    "import dill as pickle\n",
    "from toolz.sandbox.parallel import fold\n",
    "from pathos.multiprocessing import ProcessingPool as PathosPool\n",
    "from multiprocessing import Pool\n",
    "from csv import DictReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_keys(left, right):\n",
    "    return set(left.keys()).union(set(right.keys()))\n",
    "\n",
    "def prod(xs):\n",
    "    return reduce(lambda acc,nxt: acc*nxt, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob(model, k, v, label, N):\n",
    "    \"\"\"Compute probabilities for event.\"\"\"\n",
    "    Cn = model['LABELS'][label]\n",
    "    prior = Cn / N\n",
    "    evidence = model[k][v].get(label,.001) / Cn\n",
    "    return prior * evidence\n",
    "\n",
    "def _nb_suggest(ob, model, target):\n",
    "    \"\"\"maknaive Bayes prediction\"\"\"\n",
    "    ob.pop(target)\n",
    "    N = sum(model['LABELS'].values())\n",
    "    results = {}\n",
    "    for label in model['LABELS'].keys():\n",
    "        p = prod(compute_prob(model, k, v, label, N) for k, v in ob.items())\n",
    "        results[label] = p\n",
    "    return results\n",
    "\n",
    "def naive_bayes_suggest(obs, model, target):\n",
    "    \"\"\"Parallel naive Bayes prediction function\"\"\"\n",
    "    with Pool() as P:\n",
    "        f = partial(_nb_suggest, target=target)\n",
    "        return P.starmap(f, zip(obs, repeat(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_acc(acc, nxt, target):\n",
    "    label = nxt.pop(target)\n",
    "    if not acc.get('LABELS', False):\n",
    "        acc['LABELS'] = {}\n",
    "    acc['LABELS'][label] = acc['LABELS'].get(label,0) + 1\n",
    "    for k,v in nxt.items():\n",
    "        if not acc.get(k,False):\n",
    "            acc[k] = {}\n",
    "        if not acc[k].get(v, False):\n",
    "            acc[k][v] = {}\n",
    "        acc[k][v][label] = acc.get(k,{}).get(v,{}).get(label,0) + 1\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nb_comb(left, right):\n",
    "    acc = {}\n",
    "    acc['LABELS'] = {}\n",
    "    for k in unique_keys(left['LABELS'], right['LABELS']):\n",
    "        acc['LABELS'][k] = left['LABELS'].get(k,0) + right['LABELS'].get(k,0)\n",
    "    for k in unique_keys(left, right):\n",
    "        if k == 'LABELS': continue\n",
    "        acc[k] = {}\n",
    "        for v in unique_keys(left.get(k,{}), right.get(k,{})):\n",
    "            acc[k][v] = {}\n",
    "            for label in acc['LABELS']:\n",
    "                count_left = left.get(k,{}).get(v,{}).get(label,0)\n",
    "                count_right = right.get(k,{}).get(v,{}).get(label,0)\n",
    "                acc[k][v][label] = count_left + count_right\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(xs, target):\n",
    "    \"\"\"Create a naive Bayes model.\n",
    "\n",
    "\n",
    "    Inputs\n",
    "    xs: input data\n",
    "    target: target variable\n",
    "    \n",
    "    Output\n",
    "    prediction function\n",
    "\"\"\"\n",
    "    acc = partial(nb_acc, target=target)\n",
    "    with PathosPool() as P:\n",
    "        model = fold(acc, xs, {}, map=P.map, combine=_nb_comb)\n",
    "    return partial(naive_bayes_suggest, model=model, target=target)\n",
    "\n",
    "def max_prob(probs):\n",
    "    return max(((k,v) for k,v in probs.items()), key=lambda x:x[1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download [the nursery data](https://archive.ics.uci.edu/ml/machine-learning-databases/nursery/nursery.data) and assign its path to `fp` in the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"\"\n",
    "with open(fp) as f:\n",
    "    reader = DictReader(f, fieldnames=[\"parents\", \"has_nurs\", \"form\",\n",
    "                                 \"children\", \"housing\", \"finance\",\n",
    "                                 \"social\", \"health\", \"recc\"])\n",
    "    data = [row for row in reader]\n",
    "\n",
    "model = naive_bayes(data, \"recc\")\n",
    "probs = model(data)\n",
    "print(\"{}\\t\\t{}\\t{}\".format(\"Match\", \"Suggestion\", \"Actual\"))\n",
    "print(\"{}\".format(\"-\"*45))\n",
    "for i,p in enumerate(probs):\n",
    "    suggestion = max_prob(p)\n",
    "    actual = data[i]['recc']\n",
    "    match = suggestion == actual\n",
    "    print(\"{}\\t\\t{}\\t{}\".format(match, suggestion, actual))\n",
    "    if i > 25: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Read for more? Go to chapter 7!](./Ch07_notebook.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldbook",
   "language": "python",
   "name": "mldbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
